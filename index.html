<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- TODO: DESCRIPTION -->
  <meta name="description" content="description.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- TODO: TITLE -->
  <title>AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AGENTS-LLM: <u>A</u>ugmentative <u>GEN</u>eration of Challenging
            <u>T</u>raffic <u>S</u>cenarios with an Agentic <u>LLM</u> Framework</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Yu Yao</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="#">Salil Bhatnagar</a><sup>*2</sup>,</span>
            <span class="author-block">
              <a href="#">Markus Mazzola</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.lms.tf.fau.de/person/vasileios-belagiannis/">Vasileios Belagiannis</a><sup>2</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://www.gilitschenski.org/igor/">Igor Gilitschenski</a><sup>3,6</sup>,
            </span>
            <span class="author-block">
              <a href="https://palmieri.github.io">Luigi Palmieri</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://iccl.inf.tu-dresden.de/web/Simon_Razniewski/en">Simon Raznieweski</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://mh0797.github.io">Marcel Hallgarten</a><sup>1,3,5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Robert Bosch GmbH,</span>
            <span class="author-block"><sup>2</sup>University of Erlangen–Nuremberg,</span>
            <span class="author-block"><sup>3</sup>University of Toronto,</span><br>
            <span class="author-block"><sup>4</sup>ScaDS.AI & TU Dresden,</span>
            <span class="author-block"><sup>5</sup>University of Tübingen,</span>
            <span class="author-block"><sup>6</sup>Vector Institute</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- TODO arXiv pdf Link -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!-- TODO arXiv Link -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <!-- Link to Github repo -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
         <img src="./static/images/Method_overview.png" alt="Method Overview" class="image">
      <h2 class="subtitle has-text-centered">
        <!-- TODO: CAPTION -->
        Scenario modification framework. The Scenario Modifier Agent generates a modified scenario based on an original scenario and user instructions. To do this, the modifier agent has the option to make calls to an external function. An optional Quality Assurance loop can be used to evaluate the result and request corrections from the modification agent. Two alternatives exist for the QA loop: text and visual QA.
      </h2>
    </div>
  </div>
</section>

<!-- TODO, add a section with qualitative results (light) -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Rare, yet critical, scenarios pose a significant challenge in testing and evaluating autonomous driving planners.
            Relying solely on real-world driving scenes requires collecting massive datasets to capture these scenarios.
            While automatic generation of traffic scenarios appears promising, data-driven models require extensive training data and often lack fine-grained control over the output.
            Moreover, generating novel scenarios from scratch can introduce a distributional shift from the original training scenes which undermines the validity of evaluations especially for learning-based planners.
            To sidestep this, recent work proposes to generate challenging scenarios by augmenting original scenarios from the test set. However, this involves the manual augmentation of scenarios by domain experts. An approach that is unable to meet the demands for scale in the evaluation of self-driving systems.
            Therefore, this paper introduces a novel LLM-agent based framework for augmenting real-world traffic scenarios using natural language descriptions, addressing the limitations of existing methods.
            A key innovation is the use of an agentic design, enabling fine-grained control over the output and maintaining high performance even with smaller, cost-effective LLMs.
            Extensive human expert evaluation demonstrates our framework's ability to  accurately  adhere to user intent, generating high quality augmented scenarios comparable to those created manually.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Modles. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            We present an agentic LLM-based framework for augmenting real-world traffic scenarios.
            It comprises a number of LLMs taking the roles of interacting agents.
          </p>
          <p>
            First, a <strong>Scenario Modifier Agent</strong> (SMA) receives the original scenario together with a set of user instructions and generates an updated scenario containing the requested modifications.
            Advanced prompting techniques are employed to allow the SMA to understand the initial scenario and then generate modified traffic agents or objects that align with the user instructions.
            Specifically, we explore the option of allowing LLMs to make function calls in order to retrieve relevant coordinates along lanes.
          </p>
          <p>
            The output of the SMA is either directly processed by interPlan or passed through a quality assurance (QA) loop consisting of one or more <strong>Quality Assurance Agents</strong>, who's goal it is to verify if the SMA's output aligns with the user intention. 
            Two different QA strategies are explored: text-only and hybrid visual-text.
          </p>
          <p>
            In the text-only variant, a QA agent receives the initial scenario representation, the user instructions, the modified traffic agent vectors generated by the SMA and a list of common problems compiled from typical mistakes observed during initial experimentation.
            Given this input, the QA agent summarizes the initial scenario and user intent, plans verification questions that help evaluate the SMA's output and answers these questions and finally, rate the SMA's output in three categories: Compliance with User Instructions, Realism, and Logical Consistency.
            In each category, a rating from 1 to 5 is generated and if the average rating is less than 4, the QA agent generates step-by-step feedback by identifying the problem, explaining the reason behind the error and suggesting corrective actions.
            This feedback is sent back to the SMA, which regenerates the scenario.
          </p>
          <p>
            In contrast, visual QA is a multi-modal, multi-agent approach, where an LLM, the <strong>QA Engineer</strong> generates critical questions which help evaluate the SMA's output.
            Next, a vision language model, the <strong>QA Agent</strong>, receives a rendered BEV image of the modified scenario and the questions from the QA Engineer, with the task to answer these questions and retrieve relevant information about the modified scenario from the image.
            Finally, the QA Engineer utilizes the output from the QA Agent to identify mistakes in the modification work and provide feedback to the SMA.
          </p>
        </div>
      </div>
    </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Modles. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Models</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we tested three different LLMs which represent a spectrum of capabilities and costs:
            <ol>
              <li>GPT-4o representing frontier models, i.e., the best currently available LLMs.</li>
              <li>Gemini-1.5-Flash representing utility models, i.e., commercial models which are less performant but more cost effective.</li>
              <li>Llama3.1-70B representing open-weight LLMs.</li>
            </ol>
            All models are used in their pretrained form without any problem-specific fine-tuning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Models. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Modles. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            In order to test our framework, we recreate the 50 human-augmented scenarios from interPlan using the different combinations of prompting strategies and LLMs.:
            <table border="1">
              <caption>Prompting Strategies</caption>
              <thead>
                <tr>
                  <th>Prompting variant</th>
                  <th>Lane Representation</th>
                  <th>QA Loop</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>one-time-modifier (OTM)</td>
                  <td>polyline</td>
                  <td>none</td>
                </tr>
                <tr>
                  <td>function calling (FC)</td>
                  <td>Bézier curves</td>
                  <td>none</td>
                </tr>
                <tr>
                  <td>text QA (tQA)</td>
                  <td>polyline</td>
                  <td>text-only</td>
                </tr>
                <tr>
                  <td>visual QA (vQA)</td>
                  <td>polyline</td>
                  <td>visual</td>
                </tr>
              </tbody>
            </table>
          </p>
        </div>
      </div>
    </div>
    <!--/ Models. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Modles. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <h3>Displacement Error</h3>
          <figure>
            <img src="./static/images/displacement_vs_error.png" alt="displacement error" class="image">
            <figcaption class="has-text-centered">
              <!-- TODO: CAPTION -->
              Displacement Error vs. Prompting Strategy
            </figcaption>
          </figure>
          <p> 
            We first compare the generated scenarios in terms of displacement error, which is the distance between the traffic agents in the LLM-generated scenario and the same agent in the human-generated scenario from interPlan.
          </p>
          <p>
            The results show that more advanced prompting strategies are most effective in reducing the displacement error for samller LLMs, while frontier models do not benefit as much.
          </p>
        </div>
        <div class="content has-text-justified">
          <h3>Manual Analysis</h3>
          <table border="1">
            <caption>Error Count per Category</caption>
            <thead>
              <tr>
                <th>Model</th>
                <th>All &darr;</th>
                <th>Position &darr;</th>
                <th>Heading &darr;</th>
                <th>Logic&darr;</th>
              </tr>
              <tbody>
                <tr>
                  <td>GPT-4o OTM</td>
                  <td>5</td>
                  <td>3</td>
                  <td>0</td>
                  <td>2</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Flash OTM</td>
                  <td>15</td>
                  <td>6</td>
                  <td>2</td>
                  <td>7</td>
                </tr>
                <tr>
                  <td>Llama3.1-70B OTM</td>
                  <td>16</td>
                  <td>11</td>
                  <td>3</td>
                  <td>2</td>
                </tr>
              </tbody>
            </thead>
          </table>
          <p>
            In order to identify common failure cases, we render the LLM-generated scenarios as BEV images and manually inspect each sample.
            Based on this analysis, we categorise errors into three groups:
            <ul>
              <li><strong>Position Error:</strong> The traffic agent is positioned at the wrong distance or completely offroad.</li>
              <li><strong>Heading Error:</strong> The heading of the traffic agent is wrong, while the position is approximately correct.</li>
              <li><strong>Logic Error:</strong> The LLM-agent made an error during reasoning or calculation.</li>
            </ul>
          </p>
          <p>
            The above table shows the number of errors per category for each LLM with one-time-modifier prompting.
            GPT-4o makes significantly fewer errors than the two smaller models.
            In addition, ”Position Error” is the dominant error type overall and Llama the model most affected by it.
          </p>
        </div>
        <div class="content has-text-justified">
          <h3>Human Expert Ranking</h3>

          <table border="1">
            <caption>Elo from human expert ranking</caption>
            <thead>
              <tr>
                <th>Model</th>
                <th>Rank</th>
                <th>Elo &uarr;</th>
                <th>95% CI</th>
                <th>Votes</th>
              </tr>
              <tbody>
                <tr>
                  <td>interplan</td>
                  <td>1</td>
                  <td>1042</td>
                  <td>-9/+11</td>
                  <td>1960</td>
                </tr>
                <tr>
                  <td>GPT-4o OTM</td>
                  <td>1</td>
                  <td>1039</td>
                  <td>-9/+11</td>
                  <td>1720</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Flash vQA</td>
                  <td>1</td>
                  <td>1025</td>
                  <td>-12/+13</td>
                  <td>720</td>
                </tr>
                <tr>
                  <td>Llama3.1-70B tQA</td>
                  <td>3</td>
                  <td>1011</td>
                  <td>-16/+15</td>
                  <td>600</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Flash tQA</td>
                  <td>3</td>
                  <td>1003</td>
                  <td>-15/+15</td>
                  <td>600</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Flash FC</td>
                  <td>4</td>
                  <td>998</td>
                  <td>-10/+9</td>
                  <td>1360</td>
                </tr>
                <tr>
                  <td>Llama3.1-70B FC</td>
                  <td>5</td>
                  <td>984</td>
                  <td>-8/+10</td>
                  <td>1360</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Flash OTM</td>
                  <td>8</td>
                  <td>953</td>
                  <td>-12/+12</td>
                  <td>1600</td>
                </tr>
                <tr>
                  <td>Llama3.1-70B OTM</td>
                  <td>8</td>
                  <td>941</td>
                  <td>-13/+11</td>
                  <td>1600</td>
                </tr>
              </tbody>
            </thead>
          </table>
          <p>
            We conducted expert rankings based on pairwise comparisons.
            This analysis complements the displacement error metric, by providing a relative measure of how convincing the modified scenarios appear to a human expert
            Overall, 5760 pairwise comparisons from nine experts from the autonomous driving research community were collected.
            Based on this data, we computed Elo model strength.
          </p>
          <p>
            Elo assigns a numerical rating to contestants based on their performance in head-to-head matches, with the rating difference between two players determining the expected outcome.
            After each match, the winner gains points and the loser loses points, with the magnitude of the update depending on the difference between actual and expected outcomes.
          </p>
          <p>
            The results show that in a blind comparison, scenarios created using GPT-4o OTM are almost indistinguishable from the human generated scenarios from interPlan.
            At the same time, OTM with the two smaller models Gemini-1.5-Flash and Llama3.1-70B are significantly weaker in terms of Elo.
            Interestingly, FC, tQA and vQA, the human judges expressed a preference towards QA variants with vQA being almost as good as GPT-4o OTM.
            This stands in to the displacement error metrics, where FC performs better.
          </p>
        </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Modles. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Samples</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="./static/images/qualitative_samples.png" alt="qualitative samples" class="image">
          </figure>
          <p> 
            The above figure shows qualitative samples for three different scenario types.
            Ego vehicle in red, modified traffic agents in blue, drivable area in gray and walkways in olive.
            Grid represents 5 m intervals.
            <ul>
              <li>
                <strong>(a):</strong> Gemini placed the accident vehicles at the correct distance, but with an unrealistically large overlap.
              </li>
              <li>
                <strong>(b):</strong> LCTGen placed the vehicle behind the intersection and on the wrong lane. Despite moderate displacement error, this misses the intention of the user.
              </li>
              <li>
                <strong>(c):</strong> Gemini placed the pedestrian at the right distance, but facing away from the road.
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title">Code</h2>
    <p>Comming soon...</p>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- TODO: ADD bibtex -->
    <pre><code>@article{yao2025agentsllm,
  author    = {Yao, Yu and Bhatnagar, Salil and Mazzola, Markus and Belagiannis, Vasileios and Gilitschenski, Igor and Palmieri, Luigi and Raznieweski, Simon and Hallgarten, Marcel},
  title     = {AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the source code of <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
            Thanks to the authors for making the code available.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
